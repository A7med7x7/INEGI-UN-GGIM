{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxNAq3QaH4Ma",
        "outputId": "f4b373d7-f869-4173-f73d-0c830d00fb06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ],
      "source": [
        "#!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eQ1LTlCcSMx",
        "outputId": "12bc1b23-54c0-4c92-f880-93d1867281e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn import set_config\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier , IsolationForest , BaggingClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import cross_val_score , RandomizedSearchCV , StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import log_loss, make_scorer, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1X3OmQ_rNIlo"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(\"https://raw.githubusercontent.com/Yousifshaheen/INEGI-gcim-vegetation-mapping-/refs/heads/main/Train.csv\")\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/Yousifshaheen/INEGI-gcim-vegetation-mapping-/refs/heads/main/Test.csv\")\n",
        "ss = pd.read_csv(\"https://raw.githubusercontent.com/Yousifshaheen/INEGI-gcim-vegetation-mapping-/refs/heads/main/SampleSubmission.csv\")\n",
        "\n",
        "train.drop(columns = ['id'], inplace=True)\n",
        "id = test['id']\n",
        "test.drop(columns = ['id'], inplace=True)\n",
        "\n",
        "train['Target'].mod(1)\n",
        "\n",
        "# replace any missing value with the previos values\n",
        "train = train.fillna(method='ffill')\n",
        "test = test.fillna(method='ffill')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeQ9HHrr0xa_"
      },
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HXq9xqph0wJ_"
      },
      "outputs": [],
      "source": [
        "def feature_engineering(df):\n",
        "# PCAs\n",
        "  df['PCAs_mean'] = (df['PCA1'] + df['PCA2'] + df['PCA3']) / 3\n",
        "  df['REFLEC_mean'] = (df['REFLEC1'] + df['REFLEC2'] + df['REFLEC3'] + df['REFLEC4'] + df['REFLEC6']) / 5\n",
        "  df['TEMP_mean'] = (df['TMPMIN1'] + df['TMPMAX1'] ) / 2\n",
        "  df['NDVIs_mean'] = (df['NDVI1'] + df['NDVI2'] + df['NDVI3'] + df['NDVI4']) / 4\n",
        "  df['PCA_with_REFLEC2_mean'] = df['PCA2'] + df['REFLEC2']\n",
        "  df['TRI1_with_ROUGH1_mean'] = df['TRI1'] + df['ROUGH1']\n",
        "  df['PEND1_with_ROUGH1_mean'] = df['PEND1'] + df['ROUGH1']\n",
        "  df['PCA_with_REFLEC2_mean_and_TRI1_with_ROUGH1_mean_and_PEND1_with_ROUGH1_mean_mean'] = (df['PCA_with_REFLEC2_mean'] + df['TRI1_with_ROUGH1_mean'] + df['PEND1_with_ROUGH1_mean']) / 3\n",
        "\n",
        "feature_engineering(train)\n",
        "feature_engineering(test)\n",
        "\n",
        "# Drop useless features\n",
        "def drop_features(df,feature):\n",
        "  df.drop(columns = f'{feature}', inplace=True)\n",
        "drop_features(train, 'PCA1')\n",
        "drop_features(train, 'PCA3')\n",
        "drop_features(train, 'REFLEC1')\n",
        "drop_features(train, 'REFLEC2')\n",
        "drop_features(train, 'REFLEC3')\n",
        "drop_features(train, 'REFLEC4')\n",
        "drop_features(train, 'REFLEC6')\n",
        "drop_features(train, 'TRI1')\n",
        "drop_features(train, 'ROUGH1')\n",
        "# for test\n",
        "drop_features(test, 'PCA1')\n",
        "drop_features(test, 'PCA3')\n",
        "drop_features(test, 'REFLEC1')\n",
        "drop_features(test, 'REFLEC2')\n",
        "drop_features(test, 'REFLEC3')\n",
        "drop_features(test, 'REFLEC4')\n",
        "drop_features(test, 'REFLEC6')\n",
        "drop_features(test, 'TRI1')\n",
        "drop_features(test, 'ROUGH1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBIL5A3Z2zxv"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EagCmQe_tTzm"
      },
      "outputs": [],
      "source": [
        "X_train = train.copy()\n",
        "X_test = test.copy()\n",
        "X_train = X_train.drop('Target', axis=1)\n",
        "\n",
        "y_train = train['Target']\n",
        "isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "is_inlier = isolation_forest.fit_predict(X_train)\n",
        "\n",
        "\n",
        "X_train_cleaned = X_train[is_inlier == 1]\n",
        "y_train_cleaned = y_train[is_inlier == 1]\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_cleaned , y_train_cleaned, test_size=0.2, random_state=42 )\n",
        "\n",
        "\n",
        "logreg = LogisticRegression(tol = 0.0000005, max_iter=500)\n",
        "logreg_model = logreg.fit(X_train_cleaned, y_train_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewWlJVDDuPnk",
        "outputId": "7f1f872d-3004-42f0-f87b-b03a319e2500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average training score: 0.1250\n",
            "Average validation score: 0.1073\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "skf = StratifiedKFold(n_splits=5,shuffle=True ,random_state=42)\n",
        "\n",
        "X_train_cleaned = X_train_cleaned.reset_index(drop=True)\n",
        "y_train_cleaned = y_train_cleaned.reset_index(drop=True)\n",
        "\n",
        "\n",
        "for train_index, valid_index in skf.split(X_train_cleaned, y_train_cleaned):\n",
        "\n",
        "    X_train_fold, X_valid_fold = X_train_cleaned.iloc[train_index], X_train_cleaned.iloc[valid_index]\n",
        "    y_train_fold, y_valid_fold = y_train_cleaned.iloc[train_index], y_train_cleaned.iloc[valid_index]\n",
        "\n",
        "\n",
        "logreg_model = logreg.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "train_score = logreg_model.score(X_train_fold, y_train_fold)\n",
        "valid_score = logreg_model.score(X_valid_fold, y_valid_fold)\n",
        "train_results = []\n",
        "valid_results = []\n",
        "train_results.append(train_score)\n",
        "valid_results.append(valid_score)\n",
        "\n",
        "print(f'Average training score: {np.mean(train_results):.4f}')\n",
        "print(f'Average validation score: {np.mean(valid_results):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uQYrx_4juiMd",
        "outputId": "8f7a9e3a-67ac-4ff4-d641-b29b4442bb68"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "train_results = []\n",
        "valid_results = []\n",
        "\n",
        "X_train_cleaned = X_train_cleaned.reset_index(drop=True)\n",
        "y_train_cleaned = y_train_cleaned.reset_index(drop=True)\n",
        "\n",
        "for train_index, valid_index in skf.split(X_train_cleaned, y_train_cleaned):\n",
        "    X_train_fold, X_valid_fold = X_train_cleaned.iloc[train_index], X_train_cleaned.iloc[valid_index]\n",
        "    y_train_fold, y_valid_fold = y_train_cleaned.iloc[train_index], y_train_cleaned.iloc[valid_index]\n",
        "\n",
        "    num_classes = len(np.unique(y_train_fold))\n",
        "    logreg_model = LogisticRegression(tol = 0.0000005)\n",
        "    logreg_model = logreg.fit(X_train_fold, y_train_fold)\n",
        "    train_score = logreg_model.score(X_train_fold, y_train_fold)\n",
        "    valid_score = logreg_model.score(X_valid_fold, y_valid_fold)\n",
        "    train_results.append(train_score)\n",
        "    valid_results.append(valid_score)\n",
        "    pred_v = logreg_model.predict_proba(X_valid_fold)\n",
        "\n",
        "pred = logreg_model.predict_proba(X_train_cleaned)\n",
        "log_loss_value = log_loss(y_train_cleaned, pred)\n",
        "print(f\"Log Loss: {log_loss_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLaDx5nmQF1L",
        "outputId": "a478b6f5-7b99-4d4b-8502-a8f89c0ac75d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n",
            "<ipython-input-17-f54e837c861e>:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  submission[columns] = 0.001\n"
          ]
        }
      ],
      "source": [
        "submission = pd.DataFrame({'id': id})\n",
        "columns = [f'Target_{i}' for i in range(125)]\n",
        "submission[columns] = 0.001\n",
        "y_pred_proba = logreg_model.predict_proba(X_test)\n",
        "for i, class_label in enumerate(logreg_model.classes_):\n",
        "    submission['Target_' + str(class_label)] = y_pred_proba[:, i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "id": "PU_-MdLcCvoe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qHbzY4aBDSg-"
      },
      "outputs": [],
      "source": [
        "logistic_sub = submission\n",
        "logisticReg = y_pred_proba\n",
        "\n",
        "X_train = train.copy()\n",
        "X_test = test.copy()\n",
        "X_train = X_train.drop('Target', axis=1)\n",
        "y_train = train['Target']\n",
        "isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "is_inlier = isolation_forest.fit_predict(X_train)\n",
        "\n",
        "X_train_cleaned = X_train[is_inlier == 1]\n",
        "y_train_cleaned = y_train[is_inlier == 1]\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_cleaned , y_train_cleaned, test_size=0.2, random_state=42 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5731eXN0DShC",
        "outputId": "9fb186b7-bc5e-44d2-ae95-39c870964be7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:\tlearn: 3.7408519\ttotal: 538ms\tremaining: 4m 28s\n",
            "1:\tlearn: 3.6893664\ttotal: 961ms\tremaining: 3m 59s\n",
            "2:\tlearn: 3.6464575\ttotal: 1.38s\tremaining: 3m 49s\n",
            "3:\tlearn: 3.5944004\ttotal: 1.8s\tremaining: 3m 42s\n",
            "4:\tlearn: 3.5528012\ttotal: 2.26s\tremaining: 3m 43s\n",
            "5:\tlearn: 3.5146424\ttotal: 2.74s\tremaining: 3m 45s\n",
            "6:\tlearn: 3.4776141\ttotal: 3.12s\tremaining: 3m 39s\n",
            "7:\tlearn: 3.4427571\ttotal: 3.37s\tremaining: 3m 27s\n",
            "8:\tlearn: 3.4080657\ttotal: 3.64s\tremaining: 3m 18s\n",
            "9:\tlearn: 3.3769834\ttotal: 3.89s\tremaining: 3m 10s\n",
            "10:\tlearn: 3.3467252\ttotal: 4.15s\tremaining: 3m 4s\n",
            "11:\tlearn: 3.3159538\ttotal: 4.39s\tremaining: 2m 58s\n",
            "12:\tlearn: 3.2880456\ttotal: 4.7s\tremaining: 2m 56s\n",
            "13:\tlearn: 3.2611559\ttotal: 4.96s\tremaining: 2m 52s\n",
            "14:\tlearn: 3.2374750\ttotal: 5.21s\tremaining: 2m 48s\n",
            "15:\tlearn: 3.2142641\ttotal: 5.49s\tremaining: 2m 45s\n",
            "16:\tlearn: 3.1906214\ttotal: 5.75s\tremaining: 2m 43s\n",
            "17:\tlearn: 3.1689789\ttotal: 6.01s\tremaining: 2m 40s\n",
            "18:\tlearn: 3.1506230\ttotal: 6.25s\tremaining: 2m 38s\n",
            "19:\tlearn: 3.1294412\ttotal: 6.5s\tremaining: 2m 36s\n",
            "20:\tlearn: 3.1103023\ttotal: 6.77s\tremaining: 2m 34s\n",
            "21:\tlearn: 3.0927832\ttotal: 7.03s\tremaining: 2m 32s\n",
            "22:\tlearn: 3.0711068\ttotal: 7.29s\tremaining: 2m 31s\n",
            "23:\tlearn: 3.0503533\ttotal: 7.54s\tremaining: 2m 29s\n",
            "24:\tlearn: 3.0345663\ttotal: 7.81s\tremaining: 2m 28s\n",
            "25:\tlearn: 3.0183059\ttotal: 8.06s\tremaining: 2m 27s\n",
            "26:\tlearn: 2.9997471\ttotal: 8.32s\tremaining: 2m 25s\n",
            "27:\tlearn: 2.9813358\ttotal: 8.58s\tremaining: 2m 24s\n",
            "28:\tlearn: 2.9629488\ttotal: 8.86s\tremaining: 2m 23s\n",
            "29:\tlearn: 2.9482445\ttotal: 9.12s\tremaining: 2m 22s\n",
            "30:\tlearn: 2.9330064\ttotal: 9.39s\tremaining: 2m 22s\n",
            "31:\tlearn: 2.9164609\ttotal: 9.68s\tremaining: 2m 21s\n",
            "32:\tlearn: 2.8987647\ttotal: 9.95s\tremaining: 2m 20s\n",
            "33:\tlearn: 2.8850950\ttotal: 10.2s\tremaining: 2m 20s\n",
            "34:\tlearn: 2.8677809\ttotal: 10.5s\tremaining: 2m 19s\n",
            "35:\tlearn: 2.8528944\ttotal: 10.8s\tremaining: 2m 18s\n",
            "36:\tlearn: 2.8408726\ttotal: 11s\tremaining: 2m 18s\n",
            "37:\tlearn: 2.8253500\ttotal: 11.3s\tremaining: 2m 17s\n",
            "38:\tlearn: 2.8108820\ttotal: 11.5s\tremaining: 2m 16s\n",
            "39:\tlearn: 2.7953349\ttotal: 11.8s\tremaining: 2m 15s\n",
            "40:\tlearn: 2.7812382\ttotal: 12.1s\tremaining: 2m 15s\n",
            "41:\tlearn: 2.7648098\ttotal: 12.3s\tremaining: 2m 14s\n",
            "42:\tlearn: 2.7522939\ttotal: 12.6s\tremaining: 2m 13s\n",
            "43:\tlearn: 2.7415828\ttotal: 12.9s\tremaining: 2m 13s\n",
            "44:\tlearn: 2.7272110\ttotal: 13.2s\tremaining: 2m 13s\n",
            "45:\tlearn: 2.7124037\ttotal: 13.6s\tremaining: 2m 13s\n",
            "46:\tlearn: 2.6988440\ttotal: 14s\tremaining: 2m 15s\n",
            "47:\tlearn: 2.6872340\ttotal: 14.5s\tremaining: 2m 16s\n",
            "48:\tlearn: 2.6725241\ttotal: 14.9s\tremaining: 2m 17s\n",
            "49:\tlearn: 2.6572049\ttotal: 15.3s\tremaining: 2m 17s\n",
            "50:\tlearn: 2.6437081\ttotal: 15.8s\tremaining: 2m 19s\n",
            "51:\tlearn: 2.6284640\ttotal: 16.3s\tremaining: 2m 20s\n",
            "52:\tlearn: 2.6180344\ttotal: 16.8s\tremaining: 2m 22s\n",
            "53:\tlearn: 2.6059865\ttotal: 17.2s\tremaining: 2m 21s\n",
            "54:\tlearn: 2.5935969\ttotal: 17.5s\tremaining: 2m 21s\n",
            "55:\tlearn: 2.5814894\ttotal: 18s\tremaining: 2m 22s\n",
            "56:\tlearn: 2.5743538\ttotal: 18.5s\tremaining: 2m 23s\n",
            "57:\tlearn: 2.5634258\ttotal: 19.1s\tremaining: 2m 25s\n",
            "58:\tlearn: 2.5545316\ttotal: 19.5s\tremaining: 2m 25s\n",
            "59:\tlearn: 2.5428533\ttotal: 20.1s\tremaining: 2m 27s\n",
            "60:\tlearn: 2.5342567\ttotal: 20.5s\tremaining: 2m 27s\n",
            "61:\tlearn: 2.5235081\ttotal: 21s\tremaining: 2m 28s\n",
            "62:\tlearn: 2.5125003\ttotal: 21.5s\tremaining: 2m 29s\n",
            "63:\tlearn: 2.4991277\ttotal: 22s\tremaining: 2m 30s\n",
            "64:\tlearn: 2.4878197\ttotal: 22.3s\tremaining: 2m 29s\n",
            "65:\tlearn: 2.4743049\ttotal: 22.6s\tremaining: 2m 28s\n",
            "66:\tlearn: 2.4645362\ttotal: 22.8s\tremaining: 2m 27s\n",
            "67:\tlearn: 2.4540940\ttotal: 23.1s\tremaining: 2m 26s\n",
            "68:\tlearn: 2.4428188\ttotal: 23.3s\tremaining: 2m 25s\n",
            "69:\tlearn: 2.4320428\ttotal: 23.6s\tremaining: 2m 24s\n",
            "70:\tlearn: 2.4233619\ttotal: 23.8s\tremaining: 2m 24s\n",
            "71:\tlearn: 2.4140781\ttotal: 24.1s\tremaining: 2m 23s\n",
            "72:\tlearn: 2.4025963\ttotal: 24.4s\tremaining: 2m 22s\n",
            "73:\tlearn: 2.3928240\ttotal: 24.6s\tremaining: 2m 21s\n",
            "74:\tlearn: 2.3823972\ttotal: 24.9s\tremaining: 2m 20s\n",
            "75:\tlearn: 2.3731034\ttotal: 25.1s\tremaining: 2m 20s\n",
            "76:\tlearn: 2.3620259\ttotal: 25.4s\tremaining: 2m 19s\n",
            "77:\tlearn: 2.3494152\ttotal: 25.6s\tremaining: 2m 18s\n",
            "78:\tlearn: 2.3375358\ttotal: 25.9s\tremaining: 2m 17s\n",
            "79:\tlearn: 2.3288928\ttotal: 26.1s\tremaining: 2m 17s\n",
            "80:\tlearn: 2.3202468\ttotal: 26.4s\tremaining: 2m 16s\n",
            "81:\tlearn: 2.3123931\ttotal: 26.7s\tremaining: 2m 16s\n",
            "82:\tlearn: 2.3049393\ttotal: 27.1s\tremaining: 2m 16s\n",
            "83:\tlearn: 2.2972010\ttotal: 27.5s\tremaining: 2m 16s\n",
            "84:\tlearn: 2.2926159\ttotal: 28s\tremaining: 2m 16s\n",
            "85:\tlearn: 2.2846506\ttotal: 28.4s\tremaining: 2m 16s\n",
            "86:\tlearn: 2.2803554\ttotal: 28.8s\tremaining: 2m 16s\n",
            "87:\tlearn: 2.2704938\ttotal: 29.3s\tremaining: 2m 17s\n",
            "88:\tlearn: 2.2621325\ttotal: 29.8s\tremaining: 2m 17s\n",
            "89:\tlearn: 2.2557299\ttotal: 30.3s\tremaining: 2m 18s\n",
            "90:\tlearn: 2.2475261\ttotal: 30.7s\tremaining: 2m 18s\n",
            "91:\tlearn: 2.2388049\ttotal: 31.1s\tremaining: 2m 17s\n",
            "92:\tlearn: 2.2279402\ttotal: 31.6s\tremaining: 2m 18s\n",
            "93:\tlearn: 2.2182545\ttotal: 32.1s\tremaining: 2m 18s\n",
            "94:\tlearn: 2.2126896\ttotal: 32.5s\tremaining: 2m 18s\n",
            "95:\tlearn: 2.2069979\ttotal: 32.9s\tremaining: 2m 18s\n",
            "96:\tlearn: 2.2034449\ttotal: 33.3s\tremaining: 2m 18s\n",
            "97:\tlearn: 2.1969327\ttotal: 33.6s\tremaining: 2m 17s\n",
            "98:\tlearn: 2.1880441\ttotal: 33.8s\tremaining: 2m 17s\n",
            "99:\tlearn: 2.1814681\ttotal: 34.1s\tremaining: 2m 16s\n",
            "100:\tlearn: 2.1744634\ttotal: 34.3s\tremaining: 2m 15s\n",
            "101:\tlearn: 2.1695707\ttotal: 34.6s\tremaining: 2m 15s\n",
            "102:\tlearn: 2.1640873\ttotal: 34.9s\tremaining: 2m 14s\n",
            "103:\tlearn: 2.1554873\ttotal: 35.1s\tremaining: 2m 13s\n",
            "104:\tlearn: 2.1521314\ttotal: 35.4s\tremaining: 2m 13s\n",
            "105:\tlearn: 2.1451987\ttotal: 35.6s\tremaining: 2m 12s\n",
            "106:\tlearn: 2.1417741\ttotal: 35.9s\tremaining: 2m 11s\n",
            "107:\tlearn: 2.1369838\ttotal: 36.2s\tremaining: 2m 11s\n",
            "108:\tlearn: 2.1335726\ttotal: 36.4s\tremaining: 2m 10s\n",
            "109:\tlearn: 2.1291971\ttotal: 36.7s\tremaining: 2m 10s\n",
            "110:\tlearn: 2.1250440\ttotal: 36.9s\tremaining: 2m 9s\n",
            "111:\tlearn: 2.1177958\ttotal: 37.2s\tremaining: 2m 8s\n",
            "112:\tlearn: 2.1107358\ttotal: 37.4s\tremaining: 2m 8s\n",
            "113:\tlearn: 2.1054688\ttotal: 37.7s\tremaining: 2m 7s\n",
            "114:\tlearn: 2.0977610\ttotal: 37.9s\tremaining: 2m 7s\n",
            "115:\tlearn: 2.0928252\ttotal: 38.2s\tremaining: 2m 6s\n",
            "116:\tlearn: 2.0833097\ttotal: 38.4s\tremaining: 2m 5s\n",
            "117:\tlearn: 2.0719394\ttotal: 38.7s\tremaining: 2m 5s\n",
            "118:\tlearn: 2.0660586\ttotal: 39s\tremaining: 2m 4s\n",
            "119:\tlearn: 2.0598224\ttotal: 39.2s\tremaining: 2m 4s\n",
            "120:\tlearn: 2.0541241\ttotal: 39.5s\tremaining: 2m 3s\n",
            "121:\tlearn: 2.0484160\ttotal: 39.7s\tremaining: 2m 3s\n",
            "122:\tlearn: 2.0443256\ttotal: 40.1s\tremaining: 2m 2s\n",
            "123:\tlearn: 2.0374656\ttotal: 40.5s\tremaining: 2m 2s\n",
            "124:\tlearn: 2.0361861\ttotal: 41s\tremaining: 2m 2s\n",
            "125:\tlearn: 2.0292688\ttotal: 41.4s\tremaining: 2m 2s\n",
            "126:\tlearn: 2.0246818\ttotal: 41.8s\tremaining: 2m 2s\n",
            "127:\tlearn: 2.0189537\ttotal: 42.2s\tremaining: 2m 2s\n",
            "128:\tlearn: 2.0074416\ttotal: 42.7s\tremaining: 2m 2s\n",
            "129:\tlearn: 1.9989664\ttotal: 43.1s\tremaining: 2m 2s\n",
            "130:\tlearn: 1.9933049\ttotal: 43.3s\tremaining: 2m 2s\n",
            "131:\tlearn: 1.9859631\ttotal: 43.7s\tremaining: 2m 1s\n",
            "132:\tlearn: 1.9798291\ttotal: 44.2s\tremaining: 2m 1s\n",
            "133:\tlearn: 1.9716169\ttotal: 44.8s\tremaining: 2m 2s\n",
            "134:\tlearn: 1.9637676\ttotal: 45.4s\tremaining: 2m 2s\n",
            "135:\tlearn: 1.9591490\ttotal: 45.8s\tremaining: 2m 2s\n",
            "136:\tlearn: 1.9511030\ttotal: 46.4s\tremaining: 2m 2s\n",
            "137:\tlearn: 1.9478745\ttotal: 46.9s\tremaining: 2m 2s\n",
            "138:\tlearn: 1.9438813\ttotal: 47.4s\tremaining: 2m 3s\n",
            "139:\tlearn: 1.9401446\ttotal: 47.9s\tremaining: 2m 3s\n",
            "140:\tlearn: 1.9350737\ttotal: 48.4s\tremaining: 2m 3s\n",
            "141:\tlearn: 1.9280882\ttotal: 48.7s\tremaining: 2m 2s\n",
            "142:\tlearn: 1.9235809\ttotal: 49s\tremaining: 2m 2s\n",
            "143:\tlearn: 1.9185592\ttotal: 49.2s\tremaining: 2m 1s\n",
            "144:\tlearn: 1.9127713\ttotal: 49.5s\tremaining: 2m 1s\n",
            "145:\tlearn: 1.9091247\ttotal: 49.7s\tremaining: 2m\n",
            "146:\tlearn: 1.9057210\ttotal: 50s\tremaining: 2m\n",
            "147:\tlearn: 1.8973051\ttotal: 50.3s\tremaining: 1m 59s\n",
            "148:\tlearn: 1.8938230\ttotal: 50.5s\tremaining: 1m 58s\n",
            "149:\tlearn: 1.8911853\ttotal: 50.8s\tremaining: 1m 58s\n",
            "150:\tlearn: 1.8856475\ttotal: 51s\tremaining: 1m 57s\n",
            "151:\tlearn: 1.8797470\ttotal: 51.3s\tremaining: 1m 57s\n",
            "152:\tlearn: 1.8727660\ttotal: 51.5s\tremaining: 1m 56s\n",
            "153:\tlearn: 1.8679770\ttotal: 51.8s\tremaining: 1m 56s\n",
            "154:\tlearn: 1.8623403\ttotal: 52s\tremaining: 1m 55s\n",
            "155:\tlearn: 1.8581344\ttotal: 52.3s\tremaining: 1m 55s\n",
            "156:\tlearn: 1.8526307\ttotal: 52.5s\tremaining: 1m 54s\n",
            "157:\tlearn: 1.8462267\ttotal: 52.8s\tremaining: 1m 54s\n",
            "158:\tlearn: 1.8413674\ttotal: 53s\tremaining: 1m 53s\n",
            "159:\tlearn: 1.8391052\ttotal: 53.4s\tremaining: 1m 53s\n",
            "160:\tlearn: 1.8354360\ttotal: 53.8s\tremaining: 1m 53s\n",
            "161:\tlearn: 1.8331375\ttotal: 54.3s\tremaining: 1m 53s\n",
            "162:\tlearn: 1.8286363\ttotal: 54.7s\tremaining: 1m 53s\n",
            "163:\tlearn: 1.8243187\ttotal: 55.1s\tremaining: 1m 52s\n",
            "164:\tlearn: 1.8180101\ttotal: 55.6s\tremaining: 1m 52s\n",
            "165:\tlearn: 1.8108131\ttotal: 56.1s\tremaining: 1m 52s\n",
            "166:\tlearn: 1.8060360\ttotal: 56.7s\tremaining: 1m 53s\n",
            "167:\tlearn: 1.7998941\ttotal: 57.1s\tremaining: 1m 52s\n",
            "168:\tlearn: 1.7945232\ttotal: 57.6s\tremaining: 1m 52s\n",
            "169:\tlearn: 1.7900428\ttotal: 58s\tremaining: 1m 52s\n",
            "170:\tlearn: 1.7868694\ttotal: 58.4s\tremaining: 1m 52s\n",
            "171:\tlearn: 1.7839330\ttotal: 58.9s\tremaining: 1m 52s\n",
            "172:\tlearn: 1.7795474\ttotal: 59.3s\tremaining: 1m 52s\n",
            "173:\tlearn: 1.7742726\ttotal: 59.5s\tremaining: 1m 51s\n",
            "174:\tlearn: 1.7698615\ttotal: 59.8s\tremaining: 1m 51s\n",
            "175:\tlearn: 1.7645565\ttotal: 1m\tremaining: 1m 50s\n",
            "176:\tlearn: 1.7603275\ttotal: 1m\tremaining: 1m 50s\n",
            "177:\tlearn: 1.7556394\ttotal: 1m 1s\tremaining: 1m 50s\n",
            "178:\tlearn: 1.7517984\ttotal: 1m 1s\tremaining: 1m 50s\n",
            "179:\tlearn: 1.7438685\ttotal: 1m 2s\tremaining: 1m 51s\n",
            "180:\tlearn: 1.7385294\ttotal: 1m 3s\tremaining: 1m 51s\n",
            "181:\tlearn: 1.7343248\ttotal: 1m 3s\tremaining: 1m 51s\n",
            "182:\tlearn: 1.7282270\ttotal: 1m 4s\tremaining: 1m 51s\n",
            "183:\tlearn: 1.7206037\ttotal: 1m 5s\tremaining: 1m 51s\n",
            "184:\tlearn: 1.7165507\ttotal: 1m 5s\tremaining: 1m 51s\n",
            "185:\tlearn: 1.7106763\ttotal: 1m 6s\tremaining: 1m 51s\n",
            "186:\tlearn: 1.7034293\ttotal: 1m 6s\tremaining: 1m 51s\n",
            "187:\tlearn: 1.6966314\ttotal: 1m 7s\tremaining: 1m 52s\n",
            "188:\tlearn: 1.6905594\ttotal: 1m 8s\tremaining: 1m 52s\n",
            "189:\tlearn: 1.6850284\ttotal: 1m 9s\tremaining: 1m 53s\n",
            "190:\tlearn: 1.6816451\ttotal: 1m 9s\tremaining: 1m 53s\n",
            "191:\tlearn: 1.6777201\ttotal: 1m 10s\tremaining: 1m 53s\n",
            "192:\tlearn: 1.6707867\ttotal: 1m 11s\tremaining: 1m 53s\n",
            "193:\tlearn: 1.6641378\ttotal: 1m 11s\tremaining: 1m 52s\n",
            "194:\tlearn: 1.6568464\ttotal: 1m 11s\tremaining: 1m 52s\n",
            "195:\tlearn: 1.6522859\ttotal: 1m 12s\tremaining: 1m 52s\n",
            "196:\tlearn: 1.6493038\ttotal: 1m 12s\tremaining: 1m 51s\n",
            "197:\tlearn: 1.6466695\ttotal: 1m 13s\tremaining: 1m 51s\n",
            "198:\tlearn: 1.6424679\ttotal: 1m 13s\tremaining: 1m 51s\n",
            "199:\tlearn: 1.6375116\ttotal: 1m 13s\tremaining: 1m 50s\n",
            "200:\tlearn: 1.6320280\ttotal: 1m 14s\tremaining: 1m 50s\n",
            "201:\tlearn: 1.6260498\ttotal: 1m 14s\tremaining: 1m 50s\n",
            "202:\tlearn: 1.6199893\ttotal: 1m 14s\tremaining: 1m 49s\n",
            "203:\tlearn: 1.6169083\ttotal: 1m 15s\tremaining: 1m 49s\n",
            "204:\tlearn: 1.6129199\ttotal: 1m 15s\tremaining: 1m 48s\n",
            "205:\tlearn: 1.6087850\ttotal: 1m 15s\tremaining: 1m 47s\n",
            "206:\tlearn: 1.6051603\ttotal: 1m 15s\tremaining: 1m 47s\n",
            "207:\tlearn: 1.5980049\ttotal: 1m 16s\tremaining: 1m 46s\n",
            "208:\tlearn: 1.5918754\ttotal: 1m 16s\tremaining: 1m 46s\n",
            "209:\tlearn: 1.5883572\ttotal: 1m 16s\tremaining: 1m 45s\n",
            "210:\tlearn: 1.5841783\ttotal: 1m 16s\tremaining: 1m 45s\n",
            "211:\tlearn: 1.5818986\ttotal: 1m 17s\tremaining: 1m 44s\n",
            "212:\tlearn: 1.5768683\ttotal: 1m 17s\tremaining: 1m 44s\n",
            "213:\tlearn: 1.5740204\ttotal: 1m 17s\tremaining: 1m 43s\n",
            "214:\tlearn: 1.5712131\ttotal: 1m 17s\tremaining: 1m 43s\n",
            "215:\tlearn: 1.5663900\ttotal: 1m 18s\tremaining: 1m 42s\n",
            "216:\tlearn: 1.5607313\ttotal: 1m 18s\tremaining: 1m 42s\n",
            "217:\tlearn: 1.5585982\ttotal: 1m 18s\tremaining: 1m 41s\n",
            "218:\tlearn: 1.5534070\ttotal: 1m 18s\tremaining: 1m 41s\n",
            "219:\tlearn: 1.5499546\ttotal: 1m 19s\tremaining: 1m 40s\n",
            "220:\tlearn: 1.5459445\ttotal: 1m 19s\tremaining: 1m 40s\n",
            "221:\tlearn: 1.5420884\ttotal: 1m 19s\tremaining: 1m 39s\n",
            "222:\tlearn: 1.5369481\ttotal: 1m 19s\tremaining: 1m 39s\n",
            "223:\tlearn: 1.5297455\ttotal: 1m 20s\tremaining: 1m 39s\n",
            "224:\tlearn: 1.5268706\ttotal: 1m 20s\tremaining: 1m 38s\n",
            "225:\tlearn: 1.5229461\ttotal: 1m 21s\tremaining: 1m 38s\n",
            "226:\tlearn: 1.5174282\ttotal: 1m 21s\tremaining: 1m 38s\n",
            "227:\tlearn: 1.5112618\ttotal: 1m 22s\tremaining: 1m 37s\n",
            "228:\tlearn: 1.5086685\ttotal: 1m 22s\tremaining: 1m 37s\n",
            "229:\tlearn: 1.5062744\ttotal: 1m 22s\tremaining: 1m 37s\n",
            "230:\tlearn: 1.5001502\ttotal: 1m 23s\tremaining: 1m 36s\n",
            "231:\tlearn: 1.4962667\ttotal: 1m 23s\tremaining: 1m 36s\n",
            "232:\tlearn: 1.4915839\ttotal: 1m 23s\tremaining: 1m 35s\n",
            "233:\tlearn: 1.4888210\ttotal: 1m 23s\tremaining: 1m 35s\n",
            "234:\tlearn: 1.4838793\ttotal: 1m 24s\tremaining: 1m 34s\n",
            "235:\tlearn: 1.4796390\ttotal: 1m 24s\tremaining: 1m 34s\n",
            "236:\tlearn: 1.4743402\ttotal: 1m 24s\tremaining: 1m 33s\n",
            "237:\tlearn: 1.4714410\ttotal: 1m 24s\tremaining: 1m 33s\n",
            "238:\tlearn: 1.4670324\ttotal: 1m 25s\tremaining: 1m 33s\n",
            "239:\tlearn: 1.4641052\ttotal: 1m 25s\tremaining: 1m 32s\n",
            "240:\tlearn: 1.4583657\ttotal: 1m 25s\tremaining: 1m 32s\n",
            "241:\tlearn: 1.4545214\ttotal: 1m 25s\tremaining: 1m 31s\n",
            "242:\tlearn: 1.4509495\ttotal: 1m 26s\tremaining: 1m 31s\n",
            "243:\tlearn: 1.4486343\ttotal: 1m 26s\tremaining: 1m 30s\n",
            "244:\tlearn: 1.4438169\ttotal: 1m 26s\tremaining: 1m 30s\n",
            "245:\tlearn: 1.4394656\ttotal: 1m 27s\tremaining: 1m 29s\n",
            "246:\tlearn: 1.4362127\ttotal: 1m 27s\tremaining: 1m 29s\n",
            "247:\tlearn: 1.4344037\ttotal: 1m 27s\tremaining: 1m 28s\n",
            "248:\tlearn: 1.4300709\ttotal: 1m 27s\tremaining: 1m 28s\n",
            "249:\tlearn: 1.4272754\ttotal: 1m 28s\tremaining: 1m 28s\n",
            "250:\tlearn: 1.4252201\ttotal: 1m 28s\tremaining: 1m 27s\n",
            "251:\tlearn: 1.4191827\ttotal: 1m 28s\tremaining: 1m 27s\n",
            "252:\tlearn: 1.4175948\ttotal: 1m 28s\tremaining: 1m 26s\n",
            "253:\tlearn: 1.4135556\ttotal: 1m 29s\tremaining: 1m 26s\n",
            "254:\tlearn: 1.4081392\ttotal: 1m 29s\tremaining: 1m 25s\n",
            "255:\tlearn: 1.4019027\ttotal: 1m 29s\tremaining: 1m 25s\n",
            "256:\tlearn: 1.3979296\ttotal: 1m 29s\tremaining: 1m 24s\n",
            "257:\tlearn: 1.3934862\ttotal: 1m 30s\tremaining: 1m 24s\n",
            "258:\tlearn: 1.3907137\ttotal: 1m 30s\tremaining: 1m 24s\n",
            "259:\tlearn: 1.3848022\ttotal: 1m 30s\tremaining: 1m 23s\n",
            "260:\tlearn: 1.3798569\ttotal: 1m 30s\tremaining: 1m 23s\n",
            "261:\tlearn: 1.3771563\ttotal: 1m 31s\tremaining: 1m 22s\n",
            "262:\tlearn: 1.3729138\ttotal: 1m 31s\tremaining: 1m 22s\n",
            "263:\tlearn: 1.3705050\ttotal: 1m 31s\tremaining: 1m 22s\n",
            "264:\tlearn: 1.3673083\ttotal: 1m 32s\tremaining: 1m 21s\n",
            "265:\tlearn: 1.3648135\ttotal: 1m 32s\tremaining: 1m 21s\n",
            "266:\tlearn: 1.3608942\ttotal: 1m 32s\tremaining: 1m 20s\n",
            "267:\tlearn: 1.3561170\ttotal: 1m 33s\tremaining: 1m 20s\n",
            "268:\tlearn: 1.3507239\ttotal: 1m 33s\tremaining: 1m 20s\n",
            "269:\tlearn: 1.3449310\ttotal: 1m 33s\tremaining: 1m 19s\n",
            "270:\tlearn: 1.3426497\ttotal: 1m 34s\tremaining: 1m 19s\n",
            "271:\tlearn: 1.3410263\ttotal: 1m 34s\tremaining: 1m 19s\n",
            "272:\tlearn: 1.3363435\ttotal: 1m 35s\tremaining: 1m 19s\n",
            "273:\tlearn: 1.3341976\ttotal: 1m 35s\tremaining: 1m 18s\n",
            "274:\tlearn: 1.3314576\ttotal: 1m 36s\tremaining: 1m 18s\n",
            "275:\tlearn: 1.3280701\ttotal: 1m 36s\tremaining: 1m 18s\n",
            "276:\tlearn: 1.3247093\ttotal: 1m 36s\tremaining: 1m 17s\n",
            "277:\tlearn: 1.3210250\ttotal: 1m 36s\tremaining: 1m 17s\n",
            "278:\tlearn: 1.3169694\ttotal: 1m 37s\tremaining: 1m 16s\n",
            "279:\tlearn: 1.3141848\ttotal: 1m 37s\tremaining: 1m 16s\n",
            "280:\tlearn: 1.3122204\ttotal: 1m 37s\tremaining: 1m 16s\n",
            "281:\tlearn: 1.3093761\ttotal: 1m 37s\tremaining: 1m 15s\n",
            "282:\tlearn: 1.3076022\ttotal: 1m 38s\tremaining: 1m 15s\n",
            "283:\tlearn: 1.3038444\ttotal: 1m 38s\tremaining: 1m 14s\n",
            "284:\tlearn: 1.2999328\ttotal: 1m 38s\tremaining: 1m 14s\n",
            "285:\tlearn: 1.2958914\ttotal: 1m 38s\tremaining: 1m 14s\n",
            "286:\tlearn: 1.2930518\ttotal: 1m 39s\tremaining: 1m 13s\n",
            "287:\tlearn: 1.2898701\ttotal: 1m 39s\tremaining: 1m 13s\n",
            "288:\tlearn: 1.2865273\ttotal: 1m 39s\tremaining: 1m 12s\n",
            "289:\tlearn: 1.2822073\ttotal: 1m 39s\tremaining: 1m 12s\n",
            "290:\tlearn: 1.2772573\ttotal: 1m 40s\tremaining: 1m 11s\n",
            "291:\tlearn: 1.2705232\ttotal: 1m 40s\tremaining: 1m 11s\n",
            "292:\tlearn: 1.2674129\ttotal: 1m 40s\tremaining: 1m 11s\n",
            "293:\tlearn: 1.2616345\ttotal: 1m 40s\tremaining: 1m 10s\n",
            "294:\tlearn: 1.2603686\ttotal: 1m 41s\tremaining: 1m 10s\n",
            "295:\tlearn: 1.2572666\ttotal: 1m 41s\tremaining: 1m 9s\n",
            "296:\tlearn: 1.2531531\ttotal: 1m 41s\tremaining: 1m 9s\n",
            "297:\tlearn: 1.2508351\ttotal: 1m 41s\tremaining: 1m 9s\n",
            "298:\tlearn: 1.2478272\ttotal: 1m 42s\tremaining: 1m 8s\n",
            "299:\tlearn: 1.2448527\ttotal: 1m 42s\tremaining: 1m 8s\n",
            "300:\tlearn: 1.2403595\ttotal: 1m 42s\tremaining: 1m 7s\n",
            "301:\tlearn: 1.2350007\ttotal: 1m 42s\tremaining: 1m 7s\n",
            "302:\tlearn: 1.2313992\ttotal: 1m 43s\tremaining: 1m 7s\n",
            "303:\tlearn: 1.2265135\ttotal: 1m 43s\tremaining: 1m 6s\n",
            "304:\tlearn: 1.2210990\ttotal: 1m 43s\tremaining: 1m 6s\n",
            "305:\tlearn: 1.2171206\ttotal: 1m 43s\tremaining: 1m 5s\n",
            "306:\tlearn: 1.2151270\ttotal: 1m 44s\tremaining: 1m 5s\n",
            "307:\tlearn: 1.2111774\ttotal: 1m 44s\tremaining: 1m 5s\n",
            "308:\tlearn: 1.2085600\ttotal: 1m 44s\tremaining: 1m 4s\n",
            "309:\tlearn: 1.2039739\ttotal: 1m 45s\tremaining: 1m 4s\n",
            "310:\tlearn: 1.2011008\ttotal: 1m 45s\tremaining: 1m 3s\n",
            "311:\tlearn: 1.1990020\ttotal: 1m 45s\tremaining: 1m 3s\n",
            "312:\tlearn: 1.1966427\ttotal: 1m 45s\tremaining: 1m 3s\n",
            "313:\tlearn: 1.1939394\ttotal: 1m 46s\tremaining: 1m 2s\n",
            "314:\tlearn: 1.1893481\ttotal: 1m 46s\tremaining: 1m 2s\n",
            "315:\tlearn: 1.1860527\ttotal: 1m 46s\tremaining: 1m 2s\n",
            "316:\tlearn: 1.1832294\ttotal: 1m 47s\tremaining: 1m 1s\n",
            "317:\tlearn: 1.1793414\ttotal: 1m 47s\tremaining: 1m 1s\n",
            "318:\tlearn: 1.1776495\ttotal: 1m 48s\tremaining: 1m 1s\n",
            "319:\tlearn: 1.1750608\ttotal: 1m 48s\tremaining: 1m\n",
            "320:\tlearn: 1.1704780\ttotal: 1m 48s\tremaining: 1m\n",
            "321:\tlearn: 1.1663878\ttotal: 1m 49s\tremaining: 1m\n",
            "322:\tlearn: 1.1633846\ttotal: 1m 49s\tremaining: 1m\n",
            "323:\tlearn: 1.1603525\ttotal: 1m 49s\tremaining: 59.7s\n",
            "324:\tlearn: 1.1571750\ttotal: 1m 50s\tremaining: 59.3s\n",
            "325:\tlearn: 1.1526325\ttotal: 1m 50s\tremaining: 58.9s\n",
            "326:\tlearn: 1.1513185\ttotal: 1m 50s\tremaining: 58.6s\n",
            "327:\tlearn: 1.1480503\ttotal: 1m 50s\tremaining: 58.2s\n",
            "328:\tlearn: 1.1448363\ttotal: 1m 51s\tremaining: 57.8s\n",
            "329:\tlearn: 1.1417294\ttotal: 1m 51s\tremaining: 57.4s\n",
            "330:\tlearn: 1.1385465\ttotal: 1m 51s\tremaining: 57s\n",
            "331:\tlearn: 1.1353213\ttotal: 1m 51s\tremaining: 56.6s\n",
            "332:\tlearn: 1.1338302\ttotal: 1m 52s\tremaining: 56.3s\n",
            "333:\tlearn: 1.1299092\ttotal: 1m 52s\tremaining: 55.9s\n",
            "334:\tlearn: 1.1274101\ttotal: 1m 52s\tremaining: 55.5s\n",
            "335:\tlearn: 1.1231450\ttotal: 1m 52s\tremaining: 55.1s\n",
            "336:\tlearn: 1.1193426\ttotal: 1m 53s\tremaining: 54.7s\n",
            "337:\tlearn: 1.1164753\ttotal: 1m 53s\tremaining: 54.4s\n",
            "338:\tlearn: 1.1140305\ttotal: 1m 53s\tremaining: 54s\n",
            "339:\tlearn: 1.1121150\ttotal: 1m 53s\tremaining: 53.6s\n",
            "340:\tlearn: 1.1067811\ttotal: 1m 54s\tremaining: 53.2s\n",
            "341:\tlearn: 1.1051178\ttotal: 1m 54s\tremaining: 52.9s\n",
            "342:\tlearn: 1.1008467\ttotal: 1m 54s\tremaining: 52.5s\n",
            "343:\tlearn: 1.0995688\ttotal: 1m 54s\tremaining: 52.1s\n",
            "344:\tlearn: 1.0972814\ttotal: 1m 55s\tremaining: 51.7s\n",
            "345:\tlearn: 1.0945157\ttotal: 1m 55s\tremaining: 51.4s\n",
            "346:\tlearn: 1.0907706\ttotal: 1m 55s\tremaining: 51s\n",
            "347:\tlearn: 1.0886373\ttotal: 1m 55s\tremaining: 50.6s\n",
            "348:\tlearn: 1.0836478\ttotal: 1m 56s\tremaining: 50.2s\n",
            "349:\tlearn: 1.0813075\ttotal: 1m 56s\tremaining: 49.9s\n",
            "350:\tlearn: 1.0787095\ttotal: 1m 56s\tremaining: 49.5s\n",
            "351:\tlearn: 1.0757035\ttotal: 1m 56s\tremaining: 49.1s\n",
            "352:\tlearn: 1.0708740\ttotal: 1m 57s\tremaining: 48.8s\n",
            "353:\tlearn: 1.0689488\ttotal: 1m 57s\tremaining: 48.4s\n",
            "354:\tlearn: 1.0662096\ttotal: 1m 57s\tremaining: 48.1s\n",
            "355:\tlearn: 1.0621659\ttotal: 1m 57s\tremaining: 47.7s\n",
            "356:\tlearn: 1.0589167\ttotal: 1m 58s\tremaining: 47.3s\n",
            "357:\tlearn: 1.0561694\ttotal: 1m 58s\tremaining: 47s\n",
            "358:\tlearn: 1.0528434\ttotal: 1m 58s\tremaining: 46.6s\n",
            "359:\tlearn: 1.0487590\ttotal: 1m 58s\tremaining: 46.2s\n",
            "360:\tlearn: 1.0445319\ttotal: 1m 59s\tremaining: 45.9s\n",
            "361:\tlearn: 1.0421931\ttotal: 1m 59s\tremaining: 45.5s\n",
            "362:\tlearn: 1.0392268\ttotal: 1m 59s\tremaining: 45.2s\n",
            "363:\tlearn: 1.0365790\ttotal: 2m\tremaining: 44.9s\n",
            "364:\tlearn: 1.0314233\ttotal: 2m\tremaining: 44.6s\n",
            "365:\tlearn: 1.0291275\ttotal: 2m\tremaining: 44.3s\n",
            "366:\tlearn: 1.0255430\ttotal: 2m 1s\tremaining: 44s\n",
            "367:\tlearn: 1.0236305\ttotal: 2m 1s\tremaining: 43.7s\n",
            "368:\tlearn: 1.0227924\ttotal: 2m 2s\tremaining: 43.4s\n",
            "369:\tlearn: 1.0195699\ttotal: 2m 2s\tremaining: 43.1s\n",
            "370:\tlearn: 1.0152206\ttotal: 2m 2s\tremaining: 42.8s\n",
            "371:\tlearn: 1.0129094\ttotal: 2m 3s\tremaining: 42.4s\n",
            "372:\tlearn: 1.0089880\ttotal: 2m 3s\tremaining: 42s\n",
            "373:\tlearn: 1.0061988\ttotal: 2m 3s\tremaining: 41.7s\n",
            "374:\tlearn: 1.0034153\ttotal: 2m 3s\tremaining: 41.3s\n",
            "375:\tlearn: 1.0011318\ttotal: 2m 4s\tremaining: 41s\n",
            "376:\tlearn: 0.9978919\ttotal: 2m 4s\tremaining: 40.6s\n",
            "377:\tlearn: 0.9955468\ttotal: 2m 4s\tremaining: 40.2s\n",
            "378:\tlearn: 0.9921866\ttotal: 2m 4s\tremaining: 39.9s\n",
            "379:\tlearn: 0.9893509\ttotal: 2m 5s\tremaining: 39.5s\n",
            "380:\tlearn: 0.9867279\ttotal: 2m 5s\tremaining: 39.2s\n",
            "381:\tlearn: 0.9837723\ttotal: 2m 5s\tremaining: 38.8s\n",
            "382:\tlearn: 0.9813115\ttotal: 2m 6s\tremaining: 38.5s\n",
            "383:\tlearn: 0.9785727\ttotal: 2m 6s\tremaining: 38.1s\n",
            "384:\tlearn: 0.9757058\ttotal: 2m 6s\tremaining: 37.8s\n",
            "385:\tlearn: 0.9733304\ttotal: 2m 6s\tremaining: 37.4s\n",
            "386:\tlearn: 0.9719058\ttotal: 2m 6s\tremaining: 37.1s\n",
            "387:\tlearn: 0.9688400\ttotal: 2m 7s\tremaining: 36.7s\n",
            "388:\tlearn: 0.9666877\ttotal: 2m 7s\tremaining: 36.4s\n",
            "389:\tlearn: 0.9651443\ttotal: 2m 7s\tremaining: 36s\n",
            "390:\tlearn: 0.9628115\ttotal: 2m 7s\tremaining: 35.7s\n",
            "391:\tlearn: 0.9613028\ttotal: 2m 8s\tremaining: 35.3s\n",
            "392:\tlearn: 0.9595102\ttotal: 2m 8s\tremaining: 35s\n",
            "393:\tlearn: 0.9562177\ttotal: 2m 8s\tremaining: 34.6s\n",
            "394:\tlearn: 0.9536193\ttotal: 2m 8s\tremaining: 34.3s\n",
            "395:\tlearn: 0.9510587\ttotal: 2m 9s\tremaining: 33.9s\n",
            "396:\tlearn: 0.9489166\ttotal: 2m 9s\tremaining: 33.6s\n",
            "397:\tlearn: 0.9445165\ttotal: 2m 9s\tremaining: 33.2s\n",
            "398:\tlearn: 0.9423567\ttotal: 2m 9s\tremaining: 32.9s\n",
            "399:\tlearn: 0.9404008\ttotal: 2m 10s\tremaining: 32.6s\n",
            "400:\tlearn: 0.9383470\ttotal: 2m 10s\tremaining: 32.2s\n",
            "401:\tlearn: 0.9365170\ttotal: 2m 10s\tremaining: 31.9s\n",
            "402:\tlearn: 0.9345697\ttotal: 2m 10s\tremaining: 31.5s\n",
            "403:\tlearn: 0.9328397\ttotal: 2m 11s\tremaining: 31.2s\n",
            "404:\tlearn: 0.9302234\ttotal: 2m 11s\tremaining: 30.8s\n",
            "405:\tlearn: 0.9278292\ttotal: 2m 11s\tremaining: 30.5s\n",
            "406:\tlearn: 0.9257904\ttotal: 2m 11s\tremaining: 30.2s\n",
            "407:\tlearn: 0.9247716\ttotal: 2m 12s\tremaining: 29.8s\n",
            "408:\tlearn: 0.9237138\ttotal: 2m 12s\tremaining: 29.5s\n",
            "409:\tlearn: 0.9210799\ttotal: 2m 12s\tremaining: 29.1s\n",
            "410:\tlearn: 0.9192179\ttotal: 2m 13s\tremaining: 28.8s\n",
            "411:\tlearn: 0.9165983\ttotal: 2m 13s\tremaining: 28.5s\n",
            "412:\tlearn: 0.9151702\ttotal: 2m 14s\tremaining: 28.2s\n",
            "413:\tlearn: 0.9135357\ttotal: 2m 14s\tremaining: 27.9s\n",
            "414:\tlearn: 0.9121890\ttotal: 2m 14s\tremaining: 27.6s\n",
            "415:\tlearn: 0.9101315\ttotal: 2m 15s\tremaining: 27.3s\n",
            "416:\tlearn: 0.9054172\ttotal: 2m 15s\tremaining: 27s\n",
            "417:\tlearn: 0.9034988\ttotal: 2m 15s\tremaining: 26.7s\n",
            "418:\tlearn: 0.9023348\ttotal: 2m 16s\tremaining: 26.3s\n",
            "419:\tlearn: 0.9005996\ttotal: 2m 16s\tremaining: 26s\n",
            "420:\tlearn: 0.8961996\ttotal: 2m 16s\tremaining: 25.7s\n",
            "421:\tlearn: 0.8920832\ttotal: 2m 16s\tremaining: 25.3s\n",
            "422:\tlearn: 0.8902790\ttotal: 2m 17s\tremaining: 25s\n",
            "423:\tlearn: 0.8882177\ttotal: 2m 17s\tremaining: 24.6s\n",
            "424:\tlearn: 0.8858366\ttotal: 2m 17s\tremaining: 24.3s\n",
            "425:\tlearn: 0.8809028\ttotal: 2m 17s\tremaining: 24s\n",
            "426:\tlearn: 0.8795971\ttotal: 2m 18s\tremaining: 23.6s\n",
            "427:\tlearn: 0.8780959\ttotal: 2m 18s\tremaining: 23.3s\n",
            "428:\tlearn: 0.8760278\ttotal: 2m 18s\tremaining: 23s\n",
            "429:\tlearn: 0.8749263\ttotal: 2m 18s\tremaining: 22.6s\n",
            "430:\tlearn: 0.8720629\ttotal: 2m 19s\tremaining: 22.3s\n",
            "431:\tlearn: 0.8707590\ttotal: 2m 19s\tremaining: 21.9s\n",
            "432:\tlearn: 0.8679989\ttotal: 2m 19s\tremaining: 21.6s\n",
            "433:\tlearn: 0.8648200\ttotal: 2m 19s\tremaining: 21.3s\n",
            "434:\tlearn: 0.8614072\ttotal: 2m 20s\tremaining: 20.9s\n",
            "435:\tlearn: 0.8575411\ttotal: 2m 20s\tremaining: 20.6s\n",
            "436:\tlearn: 0.8546477\ttotal: 2m 20s\tremaining: 20.3s\n",
            "437:\tlearn: 0.8529394\ttotal: 2m 20s\tremaining: 19.9s\n",
            "438:\tlearn: 0.8504843\ttotal: 2m 21s\tremaining: 19.6s\n",
            "439:\tlearn: 0.8478262\ttotal: 2m 21s\tremaining: 19.3s\n",
            "440:\tlearn: 0.8457584\ttotal: 2m 21s\tremaining: 19s\n",
            "441:\tlearn: 0.8440748\ttotal: 2m 21s\tremaining: 18.6s\n",
            "442:\tlearn: 0.8421466\ttotal: 2m 22s\tremaining: 18.3s\n",
            "443:\tlearn: 0.8404233\ttotal: 2m 22s\tremaining: 18s\n",
            "444:\tlearn: 0.8390154\ttotal: 2m 22s\tremaining: 17.6s\n",
            "445:\tlearn: 0.8362692\ttotal: 2m 22s\tremaining: 17.3s\n",
            "446:\tlearn: 0.8324652\ttotal: 2m 23s\tremaining: 17s\n",
            "447:\tlearn: 0.8306576\ttotal: 2m 23s\tremaining: 16.6s\n",
            "448:\tlearn: 0.8281218\ttotal: 2m 23s\tremaining: 16.3s\n",
            "449:\tlearn: 0.8267615\ttotal: 2m 23s\tremaining: 16s\n",
            "450:\tlearn: 0.8256433\ttotal: 2m 24s\tremaining: 15.7s\n",
            "451:\tlearn: 0.8232738\ttotal: 2m 24s\tremaining: 15.3s\n",
            "452:\tlearn: 0.8203004\ttotal: 2m 24s\tremaining: 15s\n",
            "453:\tlearn: 0.8187984\ttotal: 2m 25s\tremaining: 14.7s\n",
            "454:\tlearn: 0.8178058\ttotal: 2m 25s\tremaining: 14.4s\n",
            "455:\tlearn: 0.8150015\ttotal: 2m 25s\tremaining: 14.1s\n",
            "456:\tlearn: 0.8130951\ttotal: 2m 25s\tremaining: 13.7s\n",
            "457:\tlearn: 0.8104723\ttotal: 2m 26s\tremaining: 13.4s\n",
            "458:\tlearn: 0.8080751\ttotal: 2m 26s\tremaining: 13.1s\n",
            "459:\tlearn: 0.8050350\ttotal: 2m 27s\tremaining: 12.8s\n",
            "460:\tlearn: 0.8032501\ttotal: 2m 27s\tremaining: 12.5s\n",
            "461:\tlearn: 0.8008219\ttotal: 2m 28s\tremaining: 12.2s\n",
            "462:\tlearn: 0.7993691\ttotal: 2m 28s\tremaining: 11.9s\n",
            "463:\tlearn: 0.7973987\ttotal: 2m 28s\tremaining: 11.6s\n",
            "464:\tlearn: 0.7940895\ttotal: 2m 29s\tremaining: 11.2s\n",
            "465:\tlearn: 0.7929543\ttotal: 2m 29s\tremaining: 10.9s\n",
            "466:\tlearn: 0.7908868\ttotal: 2m 29s\tremaining: 10.6s\n",
            "467:\tlearn: 0.7891824\ttotal: 2m 29s\tremaining: 10.3s\n",
            "468:\tlearn: 0.7875191\ttotal: 2m 30s\tremaining: 9.93s\n",
            "469:\tlearn: 0.7863789\ttotal: 2m 30s\tremaining: 9.6s\n",
            "470:\tlearn: 0.7849319\ttotal: 2m 30s\tremaining: 9.28s\n",
            "471:\tlearn: 0.7829868\ttotal: 2m 30s\tremaining: 8.96s\n",
            "472:\tlearn: 0.7800626\ttotal: 2m 31s\tremaining: 8.63s\n",
            "473:\tlearn: 0.7783571\ttotal: 2m 31s\tremaining: 8.31s\n",
            "474:\tlearn: 0.7760479\ttotal: 2m 31s\tremaining: 7.99s\n",
            "475:\tlearn: 0.7740242\ttotal: 2m 31s\tremaining: 7.66s\n",
            "476:\tlearn: 0.7723647\ttotal: 2m 32s\tremaining: 7.34s\n",
            "477:\tlearn: 0.7698639\ttotal: 2m 32s\tremaining: 7.02s\n",
            "478:\tlearn: 0.7684028\ttotal: 2m 32s\tremaining: 6.69s\n",
            "479:\tlearn: 0.7656080\ttotal: 2m 32s\tremaining: 6.37s\n",
            "480:\tlearn: 0.7636415\ttotal: 2m 33s\tremaining: 6.05s\n",
            "481:\tlearn: 0.7622084\ttotal: 2m 33s\tremaining: 5.73s\n",
            "482:\tlearn: 0.7596995\ttotal: 2m 33s\tremaining: 5.41s\n",
            "483:\tlearn: 0.7584599\ttotal: 2m 33s\tremaining: 5.09s\n",
            "484:\tlearn: 0.7570898\ttotal: 2m 34s\tremaining: 4.77s\n",
            "485:\tlearn: 0.7552838\ttotal: 2m 34s\tremaining: 4.45s\n",
            "486:\tlearn: 0.7539786\ttotal: 2m 34s\tremaining: 4.13s\n",
            "487:\tlearn: 0.7510105\ttotal: 2m 34s\tremaining: 3.81s\n",
            "488:\tlearn: 0.7486601\ttotal: 2m 35s\tremaining: 3.49s\n",
            "489:\tlearn: 0.7468374\ttotal: 2m 35s\tremaining: 3.17s\n",
            "490:\tlearn: 0.7456720\ttotal: 2m 35s\tremaining: 2.85s\n",
            "491:\tlearn: 0.7433451\ttotal: 2m 36s\tremaining: 2.54s\n",
            "492:\tlearn: 0.7417737\ttotal: 2m 36s\tremaining: 2.22s\n",
            "493:\tlearn: 0.7402461\ttotal: 2m 36s\tremaining: 1.9s\n",
            "494:\tlearn: 0.7383809\ttotal: 2m 36s\tremaining: 1.58s\n",
            "495:\tlearn: 0.7363396\ttotal: 2m 37s\tremaining: 1.27s\n",
            "496:\tlearn: 0.7344024\ttotal: 2m 37s\tremaining: 949ms\n",
            "497:\tlearn: 0.7308717\ttotal: 2m 37s\tremaining: 633ms\n",
            "498:\tlearn: 0.7288995\ttotal: 2m 37s\tremaining: 316ms\n",
            "499:\tlearn: 0.7268112\ttotal: 2m 38s\tremaining: 0us\n"
          ]
        }
      ],
      "source": [
        "\n",
        "catBoost = CatBoostClassifier(iterations=500,\n",
        "                           learning_rate=0.1,\n",
        "                           depth=6,\n",
        "                           class_weights=[1] * num_classes,  # Use num_classes\n",
        "                           random_seed=42)\n",
        "\n",
        "catBoost_model = catBoost.fit(X_train_cleaned, y_train_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux3Pgz_sDShD",
        "outputId": "24229249-91da-48bc-b4e5-1b0a1c3bc269"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "X_train_cleaned = X_train_cleaned.reset_index(drop=True)\n",
        "y_train_cleaned = y_train_cleaned.reset_index(drop=True)\n",
        "\n",
        "for train_index, valid_index in skf.split(X_train_cleaned, y_train_cleaned):\n",
        "\n",
        "    X_train_fold, X_valid_fold = X_train_cleaned.iloc[train_index], X_train_cleaned.iloc[valid_index]\n",
        "    y_train_fold, y_valid_fold = y_train_cleaned.iloc[train_index], y_train_cleaned.iloc[valid_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPLMHR1xDShE",
        "outputId": "2f928a03-e387-4776-8c6b-f05cf9c4b62b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:\tlearn: 3.7482767\ttotal: 265ms\tremaining: 2m 12s\n",
            "1:\tlearn: 3.7004191\ttotal: 509ms\tremaining: 2m 6s\n",
            "2:\tlearn: 3.6568182\ttotal: 756ms\tremaining: 2m 5s\n",
            "3:\tlearn: 3.6077235\ttotal: 1.02s\tremaining: 2m 6s\n",
            "4:\tlearn: 3.5636605\ttotal: 1.39s\tremaining: 2m 17s\n",
            "5:\tlearn: 3.5233059\ttotal: 1.79s\tremaining: 2m 27s\n",
            "6:\tlearn: 3.4872915\ttotal: 2.24s\tremaining: 2m 37s\n",
            "7:\tlearn: 3.4516214\ttotal: 2.65s\tremaining: 2m 42s\n",
            "8:\tlearn: 3.4155686\ttotal: 3.04s\tremaining: 2m 45s\n",
            "9:\tlearn: 3.3841220\ttotal: 3.4s\tremaining: 2m 46s\n",
            "10:\tlearn: 3.3527059\ttotal: 3.83s\tremaining: 2m 50s\n",
            "11:\tlearn: 3.3245858\ttotal: 4.28s\tremaining: 2m 53s\n",
            "12:\tlearn: 3.2952577\ttotal: 4.58s\tremaining: 2m 51s\n",
            "13:\tlearn: 3.2670315\ttotal: 4.83s\tremaining: 2m 47s\n",
            "14:\tlearn: 3.2405174\ttotal: 5.07s\tremaining: 2m 44s\n",
            "15:\tlearn: 3.2168213\ttotal: 5.33s\tremaining: 2m 41s\n",
            "16:\tlearn: 3.1920542\ttotal: 5.58s\tremaining: 2m 38s\n",
            "17:\tlearn: 3.1701051\ttotal: 5.82s\tremaining: 2m 35s\n",
            "18:\tlearn: 3.1469074\ttotal: 6.06s\tremaining: 2m 33s\n",
            "19:\tlearn: 3.1240742\ttotal: 6.31s\tremaining: 2m 31s\n",
            "20:\tlearn: 3.1036135\ttotal: 6.56s\tremaining: 2m 29s\n",
            "21:\tlearn: 3.0834704\ttotal: 6.8s\tremaining: 2m 27s\n",
            "22:\tlearn: 3.0656567\ttotal: 7.05s\tremaining: 2m 26s\n",
            "23:\tlearn: 3.0425575\ttotal: 7.3s\tremaining: 2m 24s\n",
            "24:\tlearn: 3.0258254\ttotal: 7.55s\tremaining: 2m 23s\n",
            "25:\tlearn: 3.0082749\ttotal: 7.81s\tremaining: 2m 22s\n",
            "26:\tlearn: 2.9890537\ttotal: 8.06s\tremaining: 2m 21s\n",
            "27:\tlearn: 2.9686782\ttotal: 8.33s\tremaining: 2m 20s\n",
            "28:\tlearn: 2.9508430\ttotal: 8.59s\tremaining: 2m 19s\n",
            "29:\tlearn: 2.9344662\ttotal: 8.84s\tremaining: 2m 18s\n",
            "30:\tlearn: 2.9167330\ttotal: 9.09s\tremaining: 2m 17s\n",
            "31:\tlearn: 2.8990946\ttotal: 9.34s\tremaining: 2m 16s\n",
            "32:\tlearn: 2.8813131\ttotal: 9.59s\tremaining: 2m 15s\n",
            "33:\tlearn: 2.8646085\ttotal: 9.84s\tremaining: 2m 14s\n",
            "34:\tlearn: 2.8445979\ttotal: 10.1s\tremaining: 2m 14s\n",
            "35:\tlearn: 2.8295565\ttotal: 10.3s\tremaining: 2m 13s\n",
            "36:\tlearn: 2.8115410\ttotal: 10.6s\tremaining: 2m 12s\n",
            "37:\tlearn: 2.7957170\ttotal: 10.8s\tremaining: 2m 11s\n",
            "38:\tlearn: 2.7818423\ttotal: 11.1s\tremaining: 2m 11s\n",
            "39:\tlearn: 2.7680579\ttotal: 11.3s\tremaining: 2m 10s\n",
            "40:\tlearn: 2.7563353\ttotal: 11.6s\tremaining: 2m 9s\n",
            "41:\tlearn: 2.7404692\ttotal: 11.8s\tremaining: 2m 9s\n",
            "42:\tlearn: 2.7268649\ttotal: 12.1s\tremaining: 2m 8s\n",
            "43:\tlearn: 2.7150777\ttotal: 12.4s\tremaining: 2m 8s\n",
            "44:\tlearn: 2.7003274\ttotal: 12.6s\tremaining: 2m 7s\n",
            "45:\tlearn: 2.6857234\ttotal: 12.9s\tremaining: 2m 6s\n",
            "46:\tlearn: 2.6715690\ttotal: 13.1s\tremaining: 2m 6s\n",
            "47:\tlearn: 2.6595399\ttotal: 13.3s\tremaining: 2m 5s\n",
            "48:\tlearn: 2.6443518\ttotal: 13.6s\tremaining: 2m 5s\n",
            "49:\tlearn: 2.6284632\ttotal: 13.8s\tremaining: 2m 4s\n",
            "50:\tlearn: 2.6147105\ttotal: 14.1s\tremaining: 2m 4s\n",
            "51:\tlearn: 2.6001105\ttotal: 14.3s\tremaining: 2m 3s\n",
            "52:\tlearn: 2.5884172\ttotal: 14.7s\tremaining: 2m 3s\n",
            "53:\tlearn: 2.5750239\ttotal: 15.1s\tremaining: 2m 4s\n",
            "54:\tlearn: 2.5594621\ttotal: 15.5s\tremaining: 2m 5s\n",
            "55:\tlearn: 2.5472227\ttotal: 15.9s\tremaining: 2m 6s\n",
            "56:\tlearn: 2.5341927\ttotal: 16.3s\tremaining: 2m 6s\n",
            "57:\tlearn: 2.5235292\ttotal: 16.7s\tremaining: 2m 7s\n",
            "58:\tlearn: 2.5135785\ttotal: 17.2s\tremaining: 2m 8s\n",
            "59:\tlearn: 2.4976855\ttotal: 17.6s\tremaining: 2m 8s\n",
            "60:\tlearn: 2.4858663\ttotal: 17.8s\tremaining: 2m 8s\n",
            "61:\tlearn: 2.4717866\ttotal: 18.1s\tremaining: 2m 7s\n",
            "62:\tlearn: 2.4594425\ttotal: 18.3s\tremaining: 2m 7s\n",
            "63:\tlearn: 2.4436739\ttotal: 18.6s\tremaining: 2m 6s\n",
            "64:\tlearn: 2.4348945\ttotal: 18.8s\tremaining: 2m 5s\n",
            "65:\tlearn: 2.4258057\ttotal: 19.1s\tremaining: 2m 5s\n",
            "66:\tlearn: 2.4132919\ttotal: 19.3s\tremaining: 2m 4s\n",
            "67:\tlearn: 2.4019193\ttotal: 19.6s\tremaining: 2m 4s\n",
            "68:\tlearn: 2.3892321\ttotal: 19.8s\tremaining: 2m 3s\n",
            "69:\tlearn: 2.3778012\ttotal: 20.1s\tremaining: 2m 3s\n",
            "70:\tlearn: 2.3659239\ttotal: 20.3s\tremaining: 2m 2s\n",
            "71:\tlearn: 2.3571826\ttotal: 20.6s\tremaining: 2m 2s\n",
            "72:\tlearn: 2.3449644\ttotal: 20.8s\tremaining: 2m 1s\n",
            "73:\tlearn: 2.3362971\ttotal: 21.1s\tremaining: 2m 1s\n",
            "74:\tlearn: 2.3267660\ttotal: 21.3s\tremaining: 2m\n",
            "75:\tlearn: 2.3191613\ttotal: 21.6s\tremaining: 2m\n",
            "76:\tlearn: 2.3107056\ttotal: 21.8s\tremaining: 1m 59s\n",
            "77:\tlearn: 2.3018327\ttotal: 22.1s\tremaining: 1m 59s\n",
            "78:\tlearn: 2.2899304\ttotal: 22.3s\tremaining: 1m 59s\n",
            "79:\tlearn: 2.2779578\ttotal: 22.6s\tremaining: 1m 58s\n",
            "80:\tlearn: 2.2662424\ttotal: 22.8s\tremaining: 1m 58s\n",
            "81:\tlearn: 2.2529035\ttotal: 23.1s\tremaining: 1m 57s\n",
            "82:\tlearn: 2.2442978\ttotal: 23.4s\tremaining: 1m 57s\n",
            "83:\tlearn: 2.2374568\ttotal: 23.6s\tremaining: 1m 56s\n",
            "84:\tlearn: 2.2276504\ttotal: 23.9s\tremaining: 1m 56s\n",
            "85:\tlearn: 2.2213667\ttotal: 24.1s\tremaining: 1m 56s\n",
            "86:\tlearn: 2.2122411\ttotal: 24.4s\tremaining: 1m 55s\n",
            "87:\tlearn: 2.2000669\ttotal: 24.6s\tremaining: 1m 55s\n",
            "88:\tlearn: 2.1928096\ttotal: 24.9s\tremaining: 1m 54s\n",
            "89:\tlearn: 2.1840789\ttotal: 25.1s\tremaining: 1m 54s\n",
            "90:\tlearn: 2.1735180\ttotal: 25.4s\tremaining: 1m 53s\n",
            "91:\tlearn: 2.1637830\ttotal: 25.6s\tremaining: 1m 53s\n",
            "92:\tlearn: 2.1553027\ttotal: 25.8s\tremaining: 1m 53s\n",
            "93:\tlearn: 2.1473731\ttotal: 26.1s\tremaining: 1m 52s\n",
            "94:\tlearn: 2.1391240\ttotal: 26.3s\tremaining: 1m 52s\n",
            "95:\tlearn: 2.1334041\ttotal: 26.6s\tremaining: 1m 51s\n",
            "96:\tlearn: 2.1269594\ttotal: 26.8s\tremaining: 1m 51s\n",
            "97:\tlearn: 2.1194353\ttotal: 27.1s\tremaining: 1m 51s\n",
            "98:\tlearn: 2.1127433\ttotal: 27.3s\tremaining: 1m 50s\n",
            "99:\tlearn: 2.1058685\ttotal: 27.6s\tremaining: 1m 50s\n",
            "100:\tlearn: 2.0969653\ttotal: 28s\tremaining: 1m 50s\n",
            "101:\tlearn: 2.0934508\ttotal: 28.5s\tremaining: 1m 51s\n",
            "102:\tlearn: 2.0860414\ttotal: 28.9s\tremaining: 1m 51s\n",
            "103:\tlearn: 2.0805442\ttotal: 29.3s\tremaining: 1m 51s\n",
            "104:\tlearn: 2.0735816\ttotal: 29.7s\tremaining: 1m 51s\n",
            "105:\tlearn: 2.0645697\ttotal: 30.1s\tremaining: 1m 51s\n",
            "106:\tlearn: 2.0581263\ttotal: 30.5s\tremaining: 1m 52s\n",
            "107:\tlearn: 2.0528872\ttotal: 30.9s\tremaining: 1m 52s\n",
            "108:\tlearn: 2.0477997\ttotal: 31.1s\tremaining: 1m 51s\n",
            "109:\tlearn: 2.0375632\ttotal: 31.4s\tremaining: 1m 51s\n",
            "110:\tlearn: 2.0296818\ttotal: 31.6s\tremaining: 1m 50s\n",
            "111:\tlearn: 2.0247278\ttotal: 31.9s\tremaining: 1m 50s\n",
            "112:\tlearn: 2.0156347\ttotal: 32.1s\tremaining: 1m 50s\n",
            "113:\tlearn: 2.0040885\ttotal: 32.4s\tremaining: 1m 49s\n",
            "114:\tlearn: 1.9989423\ttotal: 32.6s\tremaining: 1m 49s\n",
            "115:\tlearn: 1.9968489\ttotal: 32.9s\tremaining: 1m 48s\n",
            "116:\tlearn: 1.9930511\ttotal: 33.1s\tremaining: 1m 48s\n",
            "117:\tlearn: 1.9862990\ttotal: 33.4s\tremaining: 1m 48s\n",
            "118:\tlearn: 1.9778021\ttotal: 33.6s\tremaining: 1m 47s\n",
            "119:\tlearn: 1.9742254\ttotal: 33.9s\tremaining: 1m 47s\n",
            "120:\tlearn: 1.9678865\ttotal: 34.1s\tremaining: 1m 46s\n",
            "121:\tlearn: 1.9601026\ttotal: 34.4s\tremaining: 1m 46s\n",
            "122:\tlearn: 1.9489775\ttotal: 34.6s\tremaining: 1m 46s\n",
            "123:\tlearn: 1.9441992\ttotal: 34.8s\tremaining: 1m 45s\n",
            "124:\tlearn: 1.9412571\ttotal: 35.1s\tremaining: 1m 45s\n",
            "125:\tlearn: 1.9331726\ttotal: 35.3s\tremaining: 1m 44s\n",
            "126:\tlearn: 1.9293861\ttotal: 35.6s\tremaining: 1m 44s\n",
            "127:\tlearn: 1.9208064\ttotal: 35.8s\tremaining: 1m 44s\n",
            "128:\tlearn: 1.9126255\ttotal: 36.1s\tremaining: 1m 43s\n",
            "129:\tlearn: 1.9004443\ttotal: 36.3s\tremaining: 1m 43s\n",
            "130:\tlearn: 1.8943251\ttotal: 36.6s\tremaining: 1m 43s\n",
            "131:\tlearn: 1.8883801\ttotal: 36.8s\tremaining: 1m 42s\n",
            "132:\tlearn: 1.8826633\ttotal: 37.2s\tremaining: 1m 42s\n",
            "133:\tlearn: 1.8760177\ttotal: 37.7s\tremaining: 1m 42s\n",
            "134:\tlearn: 1.8671863\ttotal: 38.1s\tremaining: 1m 43s\n",
            "135:\tlearn: 1.8633797\ttotal: 38.6s\tremaining: 1m 43s\n",
            "136:\tlearn: 1.8553889\ttotal: 39.1s\tremaining: 1m 43s\n",
            "137:\tlearn: 1.8496579\ttotal: 39.6s\tremaining: 1m 43s\n",
            "138:\tlearn: 1.8391776\ttotal: 40.1s\tremaining: 1m 44s\n",
            "139:\tlearn: 1.8341356\ttotal: 40.6s\tremaining: 1m 44s\n",
            "140:\tlearn: 1.8264475\ttotal: 41.2s\tremaining: 1m 44s\n",
            "141:\tlearn: 1.8213970\ttotal: 41.8s\tremaining: 1m 45s\n",
            "142:\tlearn: 1.8174145\ttotal: 42.5s\tremaining: 1m 46s\n",
            "143:\tlearn: 1.8124442\ttotal: 43.3s\tremaining: 1m 46s\n",
            "144:\tlearn: 1.8069219\ttotal: 44.3s\tremaining: 1m 48s\n",
            "145:\tlearn: 1.8035226\ttotal: 44.6s\tremaining: 1m 48s\n",
            "146:\tlearn: 1.7986539\ttotal: 44.9s\tremaining: 1m 47s\n",
            "147:\tlearn: 1.7929621\ttotal: 45.1s\tremaining: 1m 47s\n",
            "148:\tlearn: 1.7855990\ttotal: 45.4s\tremaining: 1m 46s\n",
            "149:\tlearn: 1.7793277\ttotal: 45.6s\tremaining: 1m 46s\n",
            "150:\tlearn: 1.7736407\ttotal: 45.9s\tremaining: 1m 46s\n",
            "151:\tlearn: 1.7693117\ttotal: 46.1s\tremaining: 1m 45s\n",
            "152:\tlearn: 1.7638335\ttotal: 46.4s\tremaining: 1m 45s\n",
            "153:\tlearn: 1.7579121\ttotal: 46.6s\tremaining: 1m 44s\n",
            "154:\tlearn: 1.7528080\ttotal: 46.9s\tremaining: 1m 44s\n",
            "155:\tlearn: 1.7494900\ttotal: 47.1s\tremaining: 1m 43s\n",
            "156:\tlearn: 1.7463915\ttotal: 47.4s\tremaining: 1m 43s\n",
            "157:\tlearn: 1.7411511\ttotal: 47.6s\tremaining: 1m 43s\n",
            "158:\tlearn: 1.7369594\ttotal: 47.9s\tremaining: 1m 42s\n",
            "159:\tlearn: 1.7335008\ttotal: 48.1s\tremaining: 1m 42s\n",
            "160:\tlearn: 1.7244844\ttotal: 48.4s\tremaining: 1m 41s\n",
            "161:\tlearn: 1.7215207\ttotal: 48.6s\tremaining: 1m 41s\n",
            "162:\tlearn: 1.7190952\ttotal: 48.8s\tremaining: 1m 40s\n",
            "163:\tlearn: 1.7110235\ttotal: 49.1s\tremaining: 1m 40s\n",
            "164:\tlearn: 1.7078698\ttotal: 49.3s\tremaining: 1m 40s\n",
            "165:\tlearn: 1.7012358\ttotal: 49.6s\tremaining: 1m 39s\n",
            "166:\tlearn: 1.6954045\ttotal: 49.8s\tremaining: 1m 39s\n",
            "167:\tlearn: 1.6896444\ttotal: 50.1s\tremaining: 1m 38s\n",
            "168:\tlearn: 1.6856096\ttotal: 50.3s\tremaining: 1m 38s\n",
            "169:\tlearn: 1.6796778\ttotal: 50.9s\tremaining: 1m 38s\n",
            "170:\tlearn: 1.6743732\ttotal: 51.5s\tremaining: 1m 38s\n",
            "171:\tlearn: 1.6691489\ttotal: 51.9s\tremaining: 1m 39s\n",
            "172:\tlearn: 1.6650011\ttotal: 52.5s\tremaining: 1m 39s\n",
            "173:\tlearn: 1.6608464\ttotal: 53s\tremaining: 1m 39s\n",
            "174:\tlearn: 1.6539772\ttotal: 53.5s\tremaining: 1m 39s\n",
            "175:\tlearn: 1.6498745\ttotal: 53.9s\tremaining: 1m 39s\n",
            "176:\tlearn: 1.6464441\ttotal: 54.5s\tremaining: 1m 39s\n",
            "177:\tlearn: 1.6417818\ttotal: 55.2s\tremaining: 1m 39s\n",
            "178:\tlearn: 1.6342957\ttotal: 55.9s\tremaining: 1m 40s\n",
            "179:\tlearn: 1.6285135\ttotal: 56.8s\tremaining: 1m 41s\n"
          ]
        }
      ],
      "source": [
        "catBoost_model = catBoost.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "train_score = catBoost_model.score(X_train_fold, y_train_fold)\n",
        "valid_score = catBoost_model.score(X_valid_fold, y_valid_fold)\n",
        "train_results = []\n",
        "valid_results = []\n",
        "train_results.append(train_score)\n",
        "valid_results.append(valid_score)\n",
        "\n",
        "print(f'Average training score: {np.mean(train_results):.4f}')\n",
        "print(f'Average validation score: {np.mean(valid_results):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0W0gyy44DShG"
      },
      "outputs": [],
      "source": [
        "train_results = []\n",
        "valid_results = []\n",
        "\n",
        "X_train_cleaned = X_train_cleaned.reset_index(drop=True)\n",
        "y_train_cleaned = y_train_cleaned.reset_index(drop=True)\n",
        "\n",
        "for train_index, valid_index in skf.split(X_train_cleaned, y_train_cleaned):\n",
        "    X_train_fold, X_valid_fold = X_train_cleaned.iloc[train_index], X_train_cleaned.iloc[valid_index]\n",
        "    y_train_fold, y_valid_fold = y_train_cleaned.iloc[train_index], y_train_cleaned.iloc[valid_index]\n",
        "\n",
        "    num_classes = len(np.unique(y_train_fold))\n",
        "\n",
        "    catBoost_model = CatBoostClassifier(iterations=500,\n",
        "                           learning_rate=0.1,\n",
        "                           depth=6,\n",
        "                           class_weights=[1] * num_classes,\n",
        "                           random_seed=42)\n",
        "\n",
        "    catBoost_model = catBoost_model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "    train_score = catBoost_model.score(X_train_fold, y_train_fold)\n",
        "    valid_score = catBoost_model.score(X_valid_fold, y_valid_fold)\n",
        "\n",
        "    train_results.append(train_score)\n",
        "    valid_results.append(valid_score)\n",
        "\n",
        "    pred_v = catBoost_model.predict_proba(X_valid_fold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8bbfj33DShH"
      },
      "outputs": [],
      "source": [
        "all_labels = np.unique(y_train_cleaned)\n",
        "\n",
        "log_loss_value = log_loss(y_valid_fold, pred_v, labels=all_labels)\n",
        "print(f\"Log Loss: {log_loss_value}\")\n",
        "# Log Loss: 3.4167199472665493\n",
        "# Log Loss: 3.4607471766892473 = intercept_scaling=5,max_iter=1000\n",
        "# Log Loss: 3.4607471766892473 = intercept_scaling=1,max_iter=1000\n",
        "# log loss : LogisticRegression(tol = 0.003) : Log Loss: 3.361522194282241\n",
        "# log loss : LogisticRegression(tol = 0.00001) : Log Loss: 3.4020034701587254\n",
        "# log loss : LogisticRegression(tol = 0.0000001) : Log Loss: 3.3888398096979957\n",
        "# log loss : LogisticRegression(tol = 0.0000004) : Log Loss: 3.37370679509091\n",
        "# log loss : LogisticRegression(tol = 0.0000005) : Log Loss: 3.3721897021216805\n",
        "# log loss : LogisticRegression(tol = 0.0000005,max_iter=500) : Log Loss: 3.3416681438764555"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RqJ420eDShJ"
      },
      "outputs": [],
      "source": [
        "pred = catBoost_model.predict_proba(X_train_cleaned)\n",
        "catBoost_value = log_loss(y_train_cleaned, pred)\n",
        "print(f\"Log Loss: {log_loss_value}\")\n",
        "\n",
        "cat_submission = pd.DataFrame({'id': id})\n",
        "columns = [f'Target_{i}' for i in range(125)]\n",
        "cat_submission[columns] = 0.001\n",
        "\n",
        "\n",
        "y_pred_proba_cat = catBoost_model.predict_proba(X_test)\n",
        "\n",
        "for i, class_label in enumerate(catBoost_model.classes_):\n",
        "    cat_submission['Target_' + str(class_label)] = y_pred_proba_cat[:, i]\n",
        "\n",
        "catBoost_sub = cat_submission\n",
        "y_pred_proba = y_pred_proba_cat\n",
        "final = (logistic_sub.drop(columns = ['id']) + catBoost_sub.drop(columns = ['id'])) / 2\n",
        "final.insert(0, 'id', logistic_sub['id'])\n",
        "final.to_csv('../submissions/submission_part3.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
